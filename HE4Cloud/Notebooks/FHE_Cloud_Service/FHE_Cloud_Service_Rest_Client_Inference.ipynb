{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHE Cloud Service (HE4Cloud) Rest Client Inference Demonstration\n",
    "Expected RAM usage: 3 GB.  \n",
    "Expected runtime: less than 3 minutes. \n",
    "   \n",
    "System Requirements  \n",
    "The IBM Fully Homomorphic Encryption(FHE) Service is a Cloud Services accessible via REST API, Requires an internet connection to issue HTTP request to service, such as via a browser. FHE Cloud Service Supports Chrome and Firefox browsers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction\n",
    "The IBM Fully Homomorphic Encryption (FHE) Service is an early beta programme provided under the [Community Edition License](https://ibm.ent.box.com/s/zfl6rt2p09811nyy8yow8t3mpsmkmsw6) intended to help customers understand and develop use cases utilizing the power of FHE. This service enables data scientists and developers to deploy privacy preserving machine learning driven Software-as-a-Service (SaaS) applications in the Cloud.\n",
    "\n",
    "The IBM Fully Homomorphic Encryption (FHE) Service is powered by [HELayers](https://hub.docker.com/r/ibmcom/helayers-pylab) , IBM's FHE AI SDK.\n",
    "\n",
    "The underlying assumed Trust model of the deployed application is such that the browser or the client initiating the requests to the deployed application is running in a trusted environment while the deployed application in the Cloud is running in an untrusted environment\n",
    "\n",
    "Since FHE allows for arbitrary computation over encrypted data, this Service enables clients to encrypt data in a trusted environment, send it for processing in an untrusted environment, receive the encrypted results of the processing and then decrypt in the trusted environment. This ensures that data, while not in the trusted environment is always encrypted, in transit, at rest and during compute.\n",
    "\n",
    "<img src=\"https://he4cloud.com/_nuxt/img/fhe-trust-env.341e66f.png\" style=\"background-color:white; width: 80%; height: 80%\" width=\"681\" height=\"303\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flows\n",
    "### ML Model Owner Flow\n",
    "\n",
    "The ML model owner must be a registered user of FHE Cloud Service. As an ML model owner, you can deploy a model, the deployment produces a \"ML model base url\". This url endpoint exposes RESTful API that can be used to perform training and inference (prediction) on the ML model, to manage the ML model and manage its' FHE keys and retrieve usage information. The \"ML model base url\" should be published to ML model users so they can register to the ML model and use it (see ML model User Flow). You can also retrieve the \"ML model base url\" using a rest call to the FHE Cloud Service API based on ML model details you specified on deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Use Case\n",
    "This use case demonstrates how to run inference on logistic regression (LR) model. In the use case we deploy a plain LR model for fraud detection from the trusted environment to the untrusted public environment, encrypt data samples in the trusted environment, run inference in the untrusted public environment on the deployed model, receive back the inference result in trusted environment and then decrypt the results. In this use case we protect the data for inference and not the LR model (which is a public one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model User Flow\n",
    "The user must be a registered user of the FHE Cloud Service. To be able to perform operations (e.g. inference) on the ML model, the user requests the \"ML model base url\" from the owner, registers to the ML model, creates public and secret context, uploads the public context, encrypts the data using the secret context, performs inference on the ML model and decrypts the results. The user can save the secret context on his side or encrypt it and use the FHE Cloud Service API to upload and retrieve it. When the user unregisters from the ML model all the FHE Keys will be deleted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Sign In/Sign Up\n",
    "Go to https://he4cloud.com/ click on to \"Sign In/Sign Up\" button.  \n",
    "- Sign Up: If you don't have a user yet select the Sign Up option and fill up your Username, Email and Password. you will receive a confirmation code to your email to confirm your account.\n",
    "- Sign In: If you already a user please use your Username and Password to Sign In."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Token And URL\n",
    "Go to https://he4cloud.com/ and select \"API\", you will see the \"API URL\" and your \"API TOKEN\", copy and paste them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL     = \"http://172.17.0.1:5001/api/v0.1\"\n",
    "API_TOKEN   = \"eyJhbGciOiJSUzI1NiIsImtpZCI6IjI1MTA4MDU2MjIzNDYyMTk1NiJ9.eyJpc3MiOiJodHRwOi8vMTcyLjE3LjAuMTo4MDgwIiwic3ViIjoiMjUwODkwNjk4NjA4NDEwNjI4IiwiYXVkIjpbIjI1MDg5MDcwMzAzNzU5NTY1MkBoZTRjbG91ZC1wcm9qZWN0IiwiMjUwODkwNzAyNjAxMzIyNTAwIl0sImp0aSI6IjI1MTA4MDU2MTczMTMwNTQ3NiIsImV4cCI6MTcwNjE5NDg3NiwiaWF0IjoxNzA2MTUxNjc2LCJuYmYiOjE3MDYxNTE2NzZ9.TIA8qmICsdcDu6qolIQaMtjLB5HSqc6PCSmA11Jw_Ts5WurYXrnrEVUULUA4bWf2l-6Gkg9CRM96BpJnf20I_gQmbmdmf26SlqRTbVUH2pYR1RgNfbleZ7NREX_sSD5BvPwq-UGYvo9X2dTPwKy89ggkGUf6QcrI7zchu0gefM_IjdG_Gh4dWFwUUl2fwKvYkuIZ6OtrRAG7qOroPbb88XIYXen2RswIRKyVhasFfaToBtpMPJYcy_nBvJ7lobGx2MEEj-cV8mvbT1Vil4Q16jDC5vo2XY6kBGmWGWujY2ODhlXb77GG10MSfpqUJ-WKUGTEM11ug0oHuxwrnpMYZA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start with Some Imports and Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Requirements\n",
    "Make sure that you installed all the needed requiremnets (pip install requirements.txt). Also you need to install \"pyhelayers\". To get the needed \"phyelayers\" version go to https://he4cloud.com/ and select \"Help\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** GET http://172.17.0.1:5001/api/v0.1/info/version\n",
      "Response code: 200 message: {\n",
      "  \"pyhelayers version\": \"1.5.4.0\",\n",
      "  \"server version\": \"v0.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from requests_toolbelt.multipart import encoder\n",
    "import json\n",
    "import os\n",
    "import h5py\n",
    "import time\n",
    "import json\n",
    "import pyhelayers\n",
    "from importlib_metadata import version\n",
    "\n",
    "url = API_URL  + '/info/version'\n",
    "print(f'**** GET {url}')\n",
    "response = requests.get(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "server_pyhelayers_version = json.loads(response.text)[\"pyhelayers version\"]\n",
    "\n",
    "# Verify if \"pyhelayers\" is installed\n",
    "client_pyhelayers_version = version(\"pyhelayers\")\n",
    "if client_pyhelayers_version != server_pyhelayers_version:  \n",
    "    print(f'You are using pyhelayers {client_pyhelayers_version} and the server is using pyhelayers {server_pyhelayers_version}')\n",
    "    package = f'pyhelayers=={server_pyhelayers_version}'\n",
    "    raise Exception(f'The FHE Service requeries pyhelayers version {package}') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ML Model Deployment\n",
    "For this example we will use a predefined Logistic Regression model. We will load the ML model from files and deploy it to the server using the FHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** POST http://172.17.0.1:5001/api/v0.1/my_lr_fraud_model/v2/deploy_model\n",
      "Response code: 200 message: http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/\n",
      "model url: http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/\n"
     ]
    }
   ],
   "source": [
    "data_name = 'my_data'\n",
    "model_name = 'my_lr_fraud_model_inference'\n",
    "model_version = 'v2'\n",
    "model_file_1='model_file_1'\n",
    "model_hyperparams='model_hyperparams'\n",
    "model_req_filename = \"model_requirements\"\n",
    "INPUT_DIR = Path('lr_fraud_inference/')\n",
    "model_data_filename = os.path.join(INPUT_DIR, 'x_test.h5')\n",
    "model_json_filename = os.path.join(INPUT_DIR, 'model.json')\n",
    "model_requirements_filename = os.path.join(INPUT_DIR, 'requirements')\n",
    "\n",
    "# deploy URL\n",
    "url = f'{API_URL}/{model_name}/{model_version}/deploy_model'\n",
    "print(f'**** POST {url}')\n",
    "with open(model_json_filename, 'rb') as f:\n",
    "    with open(model_requirements_filename, 'rb') as ft: \n",
    "        files = encoder.MultipartEncoder({\n",
    "            model_file_1 : (model_json_filename, f, \"application/octet-stream\"),\n",
    "            \"composite\": \"NONE\",\n",
    "            model_req_filename : (model_requirements_filename, ft, \"application/octet-stream\"),\n",
    "            \"composite\": \"NONE\"\n",
    "        })\n",
    "        deploy_headers = {'Authorization': 'Bearer ' + API_TOKEN, \"Prefer\": \"respond-async\", \"Content-Type\": files.content_type}\n",
    "        response = requests.post(url=url, headers=deploy_headers, data=files)\n",
    "        response.raise_for_status()\n",
    "        print(f'Response code: {response.status_code} message: {response.text}')\n",
    "        # print(response.status_code)\n",
    "        # print(response.text)\n",
    "        model_url = response.text\n",
    "        print(f'model url: {model_url}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. User Registration \n",
    "User registration requires two steps:\n",
    "- Register user for the ML model (ML model owner provides the \"ML model base URL\")\n",
    "- Create a user profile based on user requirements. This is preparation to allow multiple profiles for a single user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Register User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** POST http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/application/register_user\n",
      "Response code: 200 message: 250890698608410628 was registered as for service 3868ea9d-89a5-4c16-9771-a1923d97d615. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register user url\n",
    "url = model_url  + 'application/register_user'\n",
    "print(f'**** POST {url}')\n",
    "response = requests.post(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Add User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** POST http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/add_profile\n",
      "Response code: 200 message: ce42a61f-af86-4768-94c9-41615ea0519f\n"
     ]
    }
   ],
   "source": [
    "# add User profile url\n",
    "url = model_url  + 'add_profile'\n",
    "print(f'**** POST {url}')\n",
    "# optimizer_requirements = json.dumps({\"batchSize\": 16})\n",
    "response = requests.post(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url) #data=optimizer_requirements)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Get FHE Profile For Current User\n",
    "FHE profile is a full list of requirements used to generate FHE context. This list is created from user requirements (used to generate user profile above) and the ML Model description. In future we will generate the FHE context directly and skip this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** GET http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/get_profile\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: INTERNAL SERVER ERROR for url: http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/get_profile",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**** GET \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m API_TOKEN}, url\u001b[38;5;241m=\u001b[39murl)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[0;32m/mnt/c/Users/857201756/Work/HE4Cloud/Dev/Git/env/he4c/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: INTERNAL SERVER ERROR for url: http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/get_profile"
     ]
    }
   ],
   "source": [
    "# get profile url\n",
    "url = model_url   + 'get_profile'\n",
    "print(f'**** GET {url}')\n",
    "response = requests.get(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "response_text = response.text\n",
    "profile = pyhelayers.HeProfile()\n",
    "profile.from_string(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Create FHE Keys From FHE Context\n",
    "FHE context is a FHE object that can generate the set of public and private FHE keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_context = pyhelayers.DefaultContext()\n",
    "\n",
    "print('>> creating new helayers context')\n",
    "pf=pyhelayers.PublicFunctions()\n",
    "client_context.init(profile.get_he_config_requirement())\n",
    "\n",
    "# Save the encryption key\n",
    "pf=pyhelayers.PublicFunctions()\n",
    "pf.clear()\n",
    "pf.encrypt=True\n",
    "public_context_buffer_enc=client_context.save_to_buffer(pf) \n",
    "print('>> encryption key Saved')\n",
    "\n",
    "    \n",
    "# Save all but the encryption key (evaluation keys)\n",
    "pf=pyhelayers.PublicFunctions()\n",
    "pf.encrypt=False\n",
    "public_context_buffer_eva=client_context.save_to_buffer()\n",
    "print('>> evaluation keys Saved')\n",
    "\n",
    "# LOCAL: save to buffer (secret_key)\n",
    "secret_key = client_context.save_secret_key(True)\n",
    "print('>> secret key Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Upload Keys\n",
    "Uploading the key is a two step procedure. First we generate presigned upload URL and use the presigned URL to upload the key.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. Upload Evaluation Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** POST http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/upload/evaluation_key_url\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: UNAUTHORIZED for url: http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/upload/evaluation_key_url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 9\u001b[0m\n\u001b[1;32m      5\u001b[0m headers \u001b[38;5;241m=\u001b[39m { \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapplication/octet-stream\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m      7\u001b[0m }\n\u001b[1;32m      8\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39murl, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m API_TOKEN})\n\u001b[0;32m----> 9\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Evaluation Key presigned upload url\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/857201756/Work/HE4Cloud/Dev/Git/env/he4c/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: UNAUTHORIZED for url: http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/upload/evaluation_key_url"
     ]
    }
   ],
   "source": [
    "iop_buffer = None\n",
    "# Evaluation Key upload url\n",
    "url = model_url  + 'upload/evaluation_key_url'\n",
    "print(f'**** POST {url}')\n",
    "headers = { \n",
    "    'Content-Type': 'application/octet-stream' \n",
    "}\n",
    "response = requests.post(url=url, headers={'Authorization' : 'Bearer ' + API_TOKEN})\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "\n",
    "# Evaluation Key presigned upload url\n",
    "presigned_upload_ulr = response.content\n",
    "print(f'**** PUT {presigned_upload_ulr}')\n",
    "response = requests.put(presigned_upload_ulr, data=public_context_buffer_eva, headers=headers)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "\n",
    "print(f'<< Evaluation Key Uploaded Successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Upload Public Key (Encryption Key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** POST http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/upload/public_key_url\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: UNAUTHORIZED for url: http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/upload/public_key_url",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**** POST \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url\u001b[38;5;241m=\u001b[39murl, headers\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAuthorization\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m API_TOKEN})\n\u001b[0;32m----> 5\u001b[0m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse code: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Public Key presigned upload url\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/857201756/Work/HE4Cloud/Dev/Git/env/he4c/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: UNAUTHORIZED for url: http://172.17.0.1:5001/api/v0.1/3868ea9d-89a5-4c16-9771-a1923d97d615/my_lr_fraud_model/v2/upload/public_key_url"
     ]
    }
   ],
   "source": [
    "# public key upload url\n",
    "url = model_url  + 'upload/public_key_url'\n",
    "print(f'**** POST {url}')\n",
    "response = requests.post(url=url, headers={'Authorization' : 'Bearer ' + API_TOKEN})\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "\n",
    "# Public Key presigned upload url\n",
    "upload_ulr = response.content\n",
    "print(f'**** PUT {upload_ulr}')\n",
    "response = requests.put(upload_ulr, data=public_context_buffer_enc, headers=headers)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "print(f'<< Public Key Uploaded Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3. Upload Secret Key\n",
    "FHE Cloud Service allows the user to upload/download secret key, it's recommended to encrypt the secret key before uploading it. There are usecases (like a browser application) when there is no secure storage on client side. The solution – encrypt secret key with an external facility (KMS of a kind), and store the encrypted key in cloud. Here we simulate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = model_url  + 'upload/encryption_seed_url'\n",
    "print(f'**** POST {url}')\n",
    "response = requests.post(url=url, headers={'Authorization' : 'Bearer ' + API_TOKEN})\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code}')\n",
    "\n",
    "# Secret Key presigned upload url\n",
    "upload_ulr = response.content\n",
    "print(f'**** PUT {upload_ulr}')\n",
    "# it's recommended to encrypt the secret key before uploading it\n",
    "response = requests.put(upload_ulr, data=secret_key, headers=headers)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code}')\n",
    "print(f'<< Encrypted Secret Key Uploaded Successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4. Get IOP\n",
    "IOP is a model description used to encrypt and decrypt data (along with the user private/public keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = model_url  + 'get_iop'\n",
    "print(f'**** GET {url}')\n",
    "response = requests.get(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code}')\n",
    "print('>> Context upload successful and IO processor object returned')\n",
    "iop_buffer = response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Encrypt Data Samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('>> Using IO Processor to encrypt data samples')\n",
    "# get encryption key\n",
    "url = model_url  + 'get_enc_key'\n",
    "print(f'**** GET {url}')\n",
    "response = requests.get(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code}')\n",
    "enc_key = response.content\n",
    "\n",
    "encrypt_context = pyhelayers.DefaultContext()\n",
    "encrypt_context.load_from_buffer(enc_key)\n",
    "\n",
    "print('>> Load iop from buffer')\n",
    "\n",
    "loaded_iop = pyhelayers.load_io_encoder(encrypt_context, iop_buffer)\n",
    "print('>> Loaded plain samples from buffer')\n",
    "\n",
    "with h5py.File(model_data_filename, \"r\") as hf:\n",
    "    keys = list(hf.keys())\n",
    "    plain_samples = hf[keys[0]][()]\n",
    "    \n",
    "print('>> Encrypting data samples')\n",
    "encrypted_data_samples = pyhelayers.EncryptedData(encrypt_context)\n",
    "loaded_iop.encode_encrypt(encrypted_data_samples, [plain_samples[0:1]])\n",
    "print('>> Plain done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Upload Encrypted Data Samples\n",
    "Uploading the Data Samples is a two step procedure. First we generate presigned upload URL and use the presigned URL to upload the Data Samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "url = model_url  + data_name + '/upload/data_url'\n",
    "print(f'**** POST {url}')\n",
    "headers = { \n",
    "    'Content-Type': 'application/octet-stream' \n",
    "}\n",
    "response = requests.post(url=url, headers={'Authorization' : 'Bearer ' + API_TOKEN})\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "\n",
    "presigned_upload_ulr = json.loads(response.content)\n",
    "print(f'<< presigned_upload_ulr {presigned_upload_ulr}')\n",
    "encrypted_data_samples_buffer=encrypted_data_samples.save_to_buffer()\n",
    "response = requests.put(presigned_upload_ulr[\"predictDataUrl\"], data=encrypted_data_samples.save_to_buffer(), headers=headers)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "print(f'<< completed uploading ecrypted data samples.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Predict\n",
    "FHE Cloud Service supports synchronous and asynchronous predict:\n",
    "- SYNC Predict:  \n",
    "    The request will return after the prediction is completed and it will include the prediction in the response content.  \n",
    "- ASYNC Predict:  \n",
    "    The request will return immediately after prediction starts and it will include the prediction proccess id in the response content. User is required to use a separate rest request to monitor prediction status by prediction proccess id. After the the user recives a completed status the prediction result can be retrived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1. Set Your Prediction Type \n",
    "Below you can toggle between 'SYNC' and 'ASYNC' to change the prediction request behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction_type can be 'SYNC' or 'ASYNC'\n",
    "prediction_type = 'ASYNC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2.A. SYNC Predict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prediction_type == 'SYNC':\n",
    "    url = model_url + 'predict/' + data_name \n",
    "    print(f'**** POST {url}')\n",
    "    response = requests.post(headers={'Authorization' : 'Bearer ' + API_TOKEN}, \n",
    "                                url=url, \n",
    "                                timeout=1000.000)\n",
    "    response.raise_for_status()\n",
    "    print(f'Response code: {response.status_code}')\n",
    "    predictions_buffer = response.content\n",
    "    print('<< prediction completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2.B. ASYNC Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if prediction_type == 'ASYNC':\n",
    "    # async predction url\n",
    "    url = model_url  + 'predict/' + data_name + '?sync=False'\n",
    "    print(f'**** POST {url}')\n",
    "    response = requests.post(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "    response.raise_for_status()\n",
    "    print(f'Response code: {response.status_code} message: {response.text}')\n",
    "\n",
    "    prediction_url = response.text\n",
    "    # async predction status url\n",
    "    url = prediction_url  + \"check_status\"\n",
    "    status = 'Running'\n",
    "    while (status == 'Running'):\n",
    "        print(f'**** GET {url}')\n",
    "        response = requests.get(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "        response.raise_for_status()\n",
    "        print(f'Response code: {response.status_code} message: {response.text}')\n",
    "        status = response.text\n",
    "        time.sleep(5)\n",
    "    \n",
    "    # get prediction url\n",
    "    url = prediction_url  + \"get_results\"\n",
    "    print(f'**** GET {url}')\n",
    "    response = requests.get(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "    response.raise_for_status()\n",
    "    print(f'Response code: {response.status_code}')\n",
    "    predictions_buffer = response.content\n",
    "    print(f'<< prediction completed: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Decrypt Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = model_url  + 'get_enc_key'\n",
    "print(f'**** GET {url}')\n",
    "response = requests.get(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code}')\n",
    "enc_key = response.content\n",
    "\n",
    "url = model_url  + 'get_enc_seed'\n",
    "print(f'**** GET {url}')\n",
    "response = requests.get(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code}')\n",
    "secret_key = response.content\n",
    "\n",
    "decrypt_context = pyhelayers.DefaultContext()\n",
    "decrypt_context.load_from_buffer(enc_key)\n",
    "\n",
    "decrypt_context.load_secret_key(secret_key, True)\n",
    "\n",
    "client_predictions = pyhelayers.load_encrypted_data(client_context,predictions_buffer) \n",
    "loaded_iop=pyhelayers.load_io_encoder(decrypt_context, iop_buffer)\n",
    "\n",
    "plain_predictions = loaded_iop.decrypt_decode_output(client_predictions)\n",
    "\n",
    "print(f'<< plain predictions: {plain_predictions}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Unregister User\n",
    "User can unregister from the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = model_url  + 'application/unregister_user'\n",
    "print(f'**** DELETE {url}')\n",
    "response = requests.delete(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "print('<< successfully unregistered user.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Undeploy Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# undeploy medel url\n",
    "url = f'{API_URL}/{model_name}/{model_version}/undeploy_model'\n",
    "print(f'**** DELETE {url}')\n",
    "response = requests.delete(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "print('<< successfully undeployed model.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
