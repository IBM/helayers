{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FHE Cloud Service (HE4Cloud) Rest Client Inference Demonstration\n",
    "Expected RAM usage: 3 GB.  \n",
    "Expected runtime: less than 3 minutes. \n",
    "   \n",
    "System Requirements  \n",
    "The IBM Fully Homomorphic Encryption(FHE) Service is a Cloud Services accessible via REST API, Requires an internet connection to issue HTTP request to service, such as via a browser. FHE Cloud Service Supports Chrome and Firefox browsers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Introduction\n",
    "The IBM Fully Homomorphic Encryption (FHE) Service is an early beta programme provided under the [Community Edition License](https://ibm.ent.box.com/s/zfl6rt2p09811nyy8yow8t3mpsmkmsw6) intended to help customers understand and develop use cases utilizing the power of FHE. This service enables data scientists and developers to deploy privacy preserving machine learning driven Software-as-a-Service (SaaS) applications in the Cloud.\n",
    "\n",
    "The IBM Fully Homomorphic Encryption (FHE) Service is powered by [HELayers](https://hub.docker.com/r/ibmcom/helayers-pylab) , IBM's FHE AI SDK.\n",
    "\n",
    "The underlying assumed Trust model of the deployed application is such that the browser or the client initiating the requests to the deployed application is running in a trusted environment while the deployed application in the Cloud is running in an untrusted environment\n",
    "\n",
    "Since FHE allows for arbitrary computation over encrypted data, this Service enables clients to encrypt data in a trusted environment, send it for processing in an untrusted environment, receive the encrypted results of the processing and then decrypt in the trusted environment. This ensures that data, while not in the trusted environment is always encrypted, in transit, at rest and during compute.\n",
    "\n",
    "<img src=\"https://he4cloud.com/_nuxt/img/fhe-trust-env.341e66f.png\" style=\"background-color:white; width: 80%; height: 80%\" width=\"681\" height=\"303\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flows\n",
    "### ML Model Owner Flow\n",
    "\n",
    "The ML model owner must be a registered user of FHE Cloud Service. As an ML model owner, you can deploy a model, the deployment produces a \"ML model base url\". This url endpoint exposes RESTful API that can be used to perform training and inference (prediction) on the ML model, to manage the ML model and manage its' FHE keys and retrieve usage information. The \"ML model base url\" should be published to ML model users so they can register to the ML model and use it (see ML model User Flow). You can also retrieve the \"ML model base url\" using a rest call to the FHE Cloud Service API based on ML model details you specified on deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Use Case\n",
    "This use case demonstrates how to run inference on logistic regression (LR) model. In the use case we deploy a plain LR model for fraud detection from the trusted environment to the untrusted public environment, encrypt data samples in the trusted environment, run inference in the untrusted public environment on the deployed model, receive back the inference result in trusted environment and then decrypt the results. In this use case we protect the data for inference and not the LR model (which is a public one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Model User Flow\n",
    "The user must be a registered user of the FHE Cloud Service. To be able to perform operations (e.g. inference) on the ML model, the user requests the \"ML model base url\" from the owner, registers to the ML model, creates public and secret context, uploads the public context, encrypts the data using the secret context, performs inference on the ML model and decrypts the results. The user can save the secret context on his side or encrypt it and use the FHE Cloud Service API to upload and retrieve it. When the user unregisters from the ML model all the FHE Keys will be deleted.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Sign In/Sign Up\n",
    "Go to https://he4cloud.com/ click on to \"Sign In/Sign Up\" button.  \n",
    "- Sign Up: If you don't have a user yet select the Sign Up option and fill up your Username, Email and Password. you will receive a confirmation code to your email to confirm your account.\n",
    "- Sign In: If you already a user please use your Username and Password to Sign In."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Token And URL\n",
    "Go to https://he4cloud.com/ and select \"API\".\n",
    "You will see the \"API URL\", your \"API TOKEN\", the \"GATEWAY API URL\" and the \"ID TOKEN\".\n",
    "copy and paste them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_URL     = \"http://172.17.0.1:5001/api/v0.1\"\n",
    "API_TOKEN   = \"eyJhbGciOiJSUzI1NiIsImtpZCI6IjI1NzkzMjIyNjc5MDk0ODg2OCJ9.eyJpc3MiOiJodHRwOi8vMTcyLjE3LjAuMTo4MDgwIiwic3ViIjoiMjU3OTMyMTc0NzMxMzEzMTU2IiwiYXVkIjpbIjI1NzkzMjE3NzE5NzU2MzkwOEBoZTRjbG91ZC1wcm9qZWN0IiwiMjU3OTMyMTc2OTQ1ODQwMTMyIl0sImp0aSI6IjI1NzkzNjAxMjg4ODUwNjM3MiIsImV4cCI6MTcxMDI4MTA0NCwiaWF0IjoxNzEwMjM3ODQ0LCJuYmYiOjE3MTAyMzc4NDR9.YKwqUlyw-ITCeQk-_aPHbRmAA-WI-Y6CQsmK8kzsGy1Slb1XbRtiiZ8a69uM0LtTVyC69UIQsTvdhK88Tbi0tOFn1AWJRgJUY9ePeXCWyepADWUH-YmICR07OU2Myo5PPS59XCKOtd0f0GA9p7ydEJ7G4xjeDcELPLDrc2pozkcjfbyy677B7uxyaRvfmWdoynZBIIRBZeVWmNzLqky2CJE93DY90ngk2j_UlPqiwt46u9iN-qH313EjaVc5N6JuOnzuCLP2_dCw6ntxEhhV_-4o5j-__-UqwUFkpFwHv7BD7K3x0ViaWMAdfr-9umhyHdl2cIXalMwpQmg-Peyp4g\"\n",
    "API_GATEWAY_URL = \"http://172.17.0.1:3002/api/v0.1\"\n",
    "ID_TOKEN    = \"eyJhbGciOiJSUzI1NiIsImtpZCI6IjI1NzkzMjIyNjc5MDk0ODg2OCJ9.eyJpc3MiOiJodHRwOi8vMTcyLjE3LjAuMTo4MDgwIiwiYXVkIjpbIjI1NzkzMjE3NzE5NzU2MzkwOEBoZTRjbG91ZC1wcm9qZWN0IiwiMjU3OTMyMTc2OTQ1ODQwMTMyIl0sImF6cCI6IjI1NzkzMjE3NzE5NzU2MzkwOEBoZTRjbG91ZC1wcm9qZWN0IiwiYXRfaGFzaCI6Imt4Ml9NS1NxbzU3cFZTejhxaXdfalEiLCJjX2hhc2giOiJZZ3hSeGdiZk12cGJwbWVQczl4ZXZnIiwiYW1yIjpbInBhc3N3b3JkIiwicHdkIl0sImV4cCI6MTcxMDI4MTA0NCwiaWF0IjoxNzEwMjM3ODQ0LCJhdXRoX3RpbWUiOjE3MTAyMzc0OTAsInN1YiI6IjI1NzkzMjE3NDczMTMxMzE1NiIsIm5hbWUiOiJhZG1pbiBhZG1pbiIsImdpdmVuX25hbWUiOiJhZG1pbiIsImZhbWlseV9uYW1lIjoiYWRtaW4iLCJwcmVmZXJyZWRfdXNlcm5hbWUiOiJhZG1pbiIsImVtYWlsIjoiYWRtaW5AZW1haWwuY29tIiwiZW1haWxfdmVyaWZpZWQiOnRydWUsInVwZGF0ZWRfYXQiOjE3MTAyMzU1NTd9.mykUlFUur5Uosh-onbuf_AC2SCiT2K-zteoCIrMqh5pwE4JPpgBDC69HTuFTTbF3zbAuuBPzEc8GPFnUCKA9PjYKaO4KQIXEEQnFntOyv9Tsdk2u_xYAhIXtHV2VUhWy-aGNqDjwsa7hjZ_RHb1Aci9jBpX4O8J0vq4bdl-sADS2DCyvU86P6jvN_gAJbC8UHpgQL2Uz-SI7oUEwhCGyT4qxE7_XPSUMgAHSljREcvX1k6iE7vlNJ48iPN9_PdaeqwZDSEQj3LpGvsD66gY3XEjYnOXJD8X26aLXDC40SXo0PR-b6_v7jZGEf88nJIQaENRrago97M6s8nJ39JX1wA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start with Some Imports and Installations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Requirements\n",
    "Make sure that you installed all the needed requiremnets (pip install requirements.txt). Also you need to install \"pyhelayers\". To get the needed \"phyelayers\" version go to https://he4cloud.com/ and select \"Help\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Import Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** GET http://172.17.0.1:5001/api/v0.1/info/version\n",
      "Response code: 200 message: {\n",
      "  \"pyhelayers version\": \"1.5.4.0\",\n",
      "  \"server version\": \"v0.1\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from requests_toolbelt.multipart import encoder\n",
    "import json\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "import pyhelayers\n",
    "from importlib_metadata import version\n",
    "\n",
    "url = API_URL  + '/info/version'\n",
    "print(f'**** GET {url}')\n",
    "response = requests.get(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "server_pyhelayers_version = json.loads(response.text)[\"pyhelayers version\"]\n",
    "\n",
    "# Verify if \"pyhelayers\" is installed\n",
    "client_pyhelayers_version = version(\"pyhelayers\")\n",
    "if client_pyhelayers_version != server_pyhelayers_version:  \n",
    "    print(f'You are using pyhelayers {client_pyhelayers_version} and the server is using pyhelayers {server_pyhelayers_version}')\n",
    "    package = f'pyhelayers=={server_pyhelayers_version}'\n",
    "    raise Exception(f'The FHE Service requeries pyhelayers version {package}') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ML Model Deployment\n",
    "For this example we will use a predefined Logistic Regression model. We will load the ML model from files and deploy it to the server using the FHE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** POST http://172.17.0.1:5001/api/v0.1/my_lr_fraud_model_inference/v2/deploy_model\n",
      "Response code: 200 message: http://172.17.0.1:5001/api/v0.1/87ef12a1-011c-4e43-92c5-158987a19c0a/my_lr_fraud_model_inference/v2/\n",
      "model url: http://172.17.0.1:5001/api/v0.1/87ef12a1-011c-4e43-92c5-158987a19c0a/my_lr_fraud_model_inference/v2/\n"
     ]
    }
   ],
   "source": [
    "data_name = 'my_data'\n",
    "model_name = 'my_lr_fraud_model_inference'\n",
    "model_version = 'v2'\n",
    "model_file_1='model_file_1'\n",
    "model_hyperparams='model_hyperparams'\n",
    "model_req_filename = \"model_requirements\"\n",
    "INPUT_DIR = Path('lr_fraud_inference/')\n",
    "model_data_filename = os.path.join(INPUT_DIR, 'x_test.h5')\n",
    "model_json_filename = os.path.join(INPUT_DIR, 'model.json')\n",
    "model_requirements_filename = os.path.join(INPUT_DIR, 'requirements')\n",
    "\n",
    "# deploy URL\n",
    "url = f'{API_URL}/{model_name}/{model_version}/deploy_model'\n",
    "print(f'**** POST {url}')\n",
    "with open(model_json_filename, 'rb') as f:\n",
    "    with open(model_requirements_filename, 'rb') as ft: \n",
    "        files = encoder.MultipartEncoder({\n",
    "            model_file_1 : (model_json_filename, f, \"application/octet-stream\"),\n",
    "            \"composite\": \"NONE\",\n",
    "            model_req_filename : (model_requirements_filename, ft, \"application/octet-stream\"),\n",
    "            \"composite\": \"NONE\"\n",
    "        })\n",
    "        deploy_headers = {'Authorization': 'Bearer ' + API_TOKEN, \"Prefer\": \"respond-async\", \"Content-Type\": files.content_type}\n",
    "        response = requests.post(url=url, headers=deploy_headers, data=files)\n",
    "        response.raise_for_status()\n",
    "        print(f'Response code: {response.status_code} message: {response.text}')\n",
    "        # print(response.status_code)\n",
    "        # print(response.text)\n",
    "        model_url = response.text\n",
    "        print(f'model url: {model_url}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. User Registration \n",
    "User registration requires two steps:\n",
    "- Register user for the ML model (ML model owner provides the \"ML model base URL\")\n",
    "- Create a user profile based on user requirements. This is preparation to allow multiple profiles for a single user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Register User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** POST http://172.17.0.1:5001/api/v0.1/87ef12a1-011c-4e43-92c5-158987a19c0a/my_lr_fraud_model_inference/v2/application/register_user\n",
      "Response code: 200 message: 257932174731313156 was registered as for service 87ef12a1-011c-4e43-92c5-158987a19c0a. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# register user url\n",
    "url = model_url  + 'application/register_user'\n",
    "print(f'**** POST {url}')\n",
    "response = requests.post(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2. Add User Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** POST http://172.17.0.1:5001/api/v0.1/87ef12a1-011c-4e43-92c5-158987a19c0a/my_lr_fraud_model_inference/v2/add_profile\n",
      "Response code: 200 message: 529d98e3-6238-48c3-a47d-6cdd30b000d9\n"
     ]
    }
   ],
   "source": [
    "# add User profile url\n",
    "url = model_url  + 'add_profile'\n",
    "print(f'**** POST {url}')\n",
    "# optimizer_requirements = json.dumps({\"batchSize\": 16})\n",
    "response = requests.post(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url) #data=optimizer_requirements)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3. Get FHE Profile For Current User\n",
    "FHE profile is a full list of requirements used to generate FHE context. This list is created from user requirements (used to generate user profile above) and the ML Model description. In future we will generate the FHE context directly and skip this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** POST http://172.17.0.1:3002/api/v0.1/keys/generate/profile\n",
      "Response code: 200\n"
     ]
    }
   ],
   "source": [
    "# get profile and create keys\n",
    "\n",
    "url = API_GATEWAY_URL + '/keys/generate/profile'\n",
    "\n",
    "id_token=ID_TOKEN\n",
    "server_token=API_TOKEN\n",
    "model_id = model_url.replace(API_URL, \"\")[:-1]\n",
    "\n",
    "payload = { 'model_id': model_id,\n",
    "            'server_token': API_TOKEN,\n",
    "            'id_token': ID_TOKEN\n",
    "            }\n",
    "headers = {}\n",
    "\n",
    "print(f'**** POST {url}')\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "response.raise_for_status()\n",
    "\n",
    "print(f'Response code: {response.status_code}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** POST http://172.17.0.1:3002/api/v0.1/encrypt/data/predict\n",
      "Response code: 200\n",
      "Prediction response: [[-4.52843817e-02]\n",
      " [-3.42865788e+00]\n",
      " [-4.97556083e-02]\n",
      " [-3.75140362e-01]\n",
      " [-3.22944083e-03]\n",
      " [-4.50725889e+00]\n",
      " [-2.20934504e-01]\n",
      " [-3.76402971e-03]\n",
      " [-7.47906885e-02]\n",
      " [-5.48515386e-01]\n",
      " [-2.03752982e-03]\n",
      " [-6.79043068e-01]\n",
      " [-2.18142207e+00]\n",
      " [-8.31619930e-01]\n",
      " [-1.59530198e-01]\n",
      " [-3.39411449e-01]\n",
      " [-8.72607304e-04]\n",
      " [-2.63926415e+00]\n",
      " [-1.73319219e+00]\n",
      " [-1.50972469e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Upload data for encryption and prediction\n",
    "\n",
    "url = API_GATEWAY_URL + '/encrypt/data/predict'\n",
    "id_token=ID_TOKEN\n",
    "server_token=API_TOKEN\n",
    "model_id = model_url.replace(API_URL, \"\")[:-1]\n",
    "payload = { 'model_id': model_id,\n",
    "            'server_token': API_TOKEN,\n",
    "            'id_token': ID_TOKEN\n",
    "            }\n",
    "\n",
    "print(f'**** POST {url}')\n",
    "with open(model_data_filename, 'rb') as pf: \n",
    "    files = {\n",
    "        \"data_file\" : (model_data_filename, pf, \"application/octet-stream\"),\n",
    "        \"composite\": \"NONE\"\n",
    "    }\n",
    "    headers = {}\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "    response.raise_for_status()\n",
    "    print(f'Response code: {response.status_code}')\n",
    "    \n",
    "    plain_predictions = np.array(json.loads(json.loads(response.text))[\"result\"])\n",
    "    print(f'Prediction response: {plain_predictions.reshape(plain_predictions.shape[0], 1)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Predict\n",
    "FHE Cloud Service supports synchronous and asynchronous predict:\n",
    "- SYNC Predict:  \n",
    "    The request will return after the prediction is completed and it will include the prediction in the response content.  \n",
    "- ASYNC Predict:  \n",
    "    The request will return immediately after prediction starts and it will include the prediction proccess id in the response content. User is required to use a separate rest request to monitor prediction status by prediction proccess id. After the the user recives a completed status the prediction result can be retrived."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.1. Set Your Prediction Type \n",
    "Below you can toggle between 'SYNC' and 'ASYNC' to change the prediction request behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Unregister User\n",
    "User can unregister from the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** DELETE http://172.17.0.1:5001/api/v0.1/87ef12a1-011c-4e43-92c5-158987a19c0a/my_lr_fraud_model_inference/v2/application/unregister_user\n",
      "Response code: 200 message: Unregistered user\n",
      "<< successfully unregistered user.\n"
     ]
    }
   ],
   "source": [
    "url = model_url  + 'application/unregister_user'\n",
    "print(f'**** DELETE {url}')\n",
    "response = requests.delete(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "print('<< successfully unregistered user.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Undeploy Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** DELETE http://172.17.0.1:5001/api/v0.1/my_lr_fraud_model_inference/v2/undeploy_model\n",
      "Response code: 200 message: Success\n",
      "<< successfully undeployed model.\n"
     ]
    }
   ],
   "source": [
    "# undeploy medel url\n",
    "url = f'{API_URL}/{model_name}/{model_version}/undeploy_model'\n",
    "print(f'**** DELETE {url}')\n",
    "response = requests.delete(headers={'Authorization' : 'Bearer ' + API_TOKEN}, url=url)\n",
    "response.raise_for_status()\n",
    "print(f'Response code: {response.status_code} message: {response.text}')\n",
    "print('<< successfully undeployed model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO DELETE THIS CODE\n",
    "# import h5py\n",
    "# with h5py.File(\"/home/daherd/workspace/public/helayers/HE4Cloud/Notebooks/FHE_Cloud_Service/lr_fraud_inference/x_test.h5\", 'r') as input_file:\n",
    "#     # Open a new .h5 file where the first 20 entries of each dataset will be stored\n",
    "#     with h5py.File(\"/home/daherd/workspace/public/helayers/HE4Cloud/Notebooks/FHE_Cloud_Service/lr_fraud_inference/x_test_short.h5\", 'w') as output_file:\n",
    "#         # Iterate over datasets in the input file\n",
    "#         for dataset_name in input_file:\n",
    "#             # Read the full dataset from the input file\n",
    "#             data = input_file[dataset_name][:]\n",
    "#             # Take only the first 20 entries of the dataset\n",
    "#             truncated_data = data[:20]\n",
    "#             # Create a new dataset in the output file with the truncated data\n",
    "#             output_file.create_dataset(dataset_name, data=truncated_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
