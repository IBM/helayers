{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart disease example\n",
    "\n",
    "This notebook shows an example of training and running a model that classifies heart diseases.\n",
    "It is based on the <a herf=\"https://archive.ics.uci.edu/ml/datasets/Heart+Disease\">Heart Disease dataset</a> from the <a href=\"https://archive.ics.uci.edu/\">UCI</a> repository. For privacy reasons, names and social security numbers (SSNs) of patients were removed from the dataset, and were replaced with dummy values.\n",
    "\n",
    "Following the UCI description, this demonstration only uses 14 attributes out of the 76 attributes listed to predict whether a patient will have a heart attack. The target field is an integer value from 0 (no attack) to 4.\n",
    "\n",
    "The demonstration uses a 3 layer neural network (NN): FC(50) --> Square activation --> FC(1)\n",
    "\n",
    "The required estimated memory is: model (4.46MB), input (3.67MB), output (0.13MB), and context (8.18MB).\n",
    "\n",
    "We start by importing the required source packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 20:31:01.427838: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-06-07 20:31:01.427872: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import utils, callbacks, losses\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from  preprocessor import Preprocessor\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "Please refer to the dataset <a href=\"https://archive.ics.uci.edu/ml/datasets/Heart+Disease\">documentation</a> for the complete list of attributes and their description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (303, 14)\n",
      "Age                          int64\n",
      "Sex                          int64\n",
      "Chest_pain                   int64\n",
      "Resting_blood_pressure       int64\n",
      "Cholesterol                  int64\n",
      "Fasting_blood_sugar          int64\n",
      "ECG_results                  int64\n",
      "Maximum_heart_rate           int64\n",
      "Exercise_induced_angina      int64\n",
      "ST_depression              float64\n",
      "ST_slope                     int64\n",
      "Major_vessels                int64\n",
      "Thalassemia_types            int64\n",
      "Heart_attack                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"datasets/heart_disease.csv\")\n",
    "\n",
    "df.rename(columns={'age': 'Age', 'sex': 'Sex', 'cp': 'Chest_pain', 'trestbps': 'Resting_blood_pressure',\n",
    "                       'chol': 'Cholesterol', 'fbs': 'Fasting_blood_sugar',\n",
    "                       'restecg': 'ECG_results', 'thalach': 'Maximum_heart_rate',\n",
    "                       'exang': 'Exercise_induced_angina', 'oldpeak': 'ST_depression', 'ca': 'Major_vessels',\n",
    "                       'thal': 'Thalassemia_types', 'target': 'Heart_attack', 'slope': 'ST_slope'}, inplace=True)\n",
    "\n",
    "print(f'data shape: {df.shape}')\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "We first convert the categorial features (in the table below) to indicator vectors. \n",
    "\n",
    "|Attributes|Description|Values|\n",
    "|---|---|---|\n",
    "|Chest_pain| The chest pain type | Typical angina (1), Atypical angina (2), Non-anginal pain (3), Asymptomatic (4)|\n",
    "|Thalassemia_types| | Normal (3), Fixed defect (6), Reversable defect (7)|\n",
    "|ECG_results| Resting electrocardiographic results|<ul><li>Normal (0)</li></ul><ul><li>Having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV) (1)</li></ul><ul><li> Showing probable or definite left ventricular hypertrophy by Estes' criteria (2)</li></ul>|\n",
    "|ST_slope| The slope of the peak exercise ST segment| Upsloping (1), Flat (2), Downsloping (3)|\n",
    "|Major_vessels | The number of major vessels (0-3) colored by flourosopy | 0-3 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, we split every row into its target value (y) and predicates (X)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df\n",
    "y = df['Heart_attack']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into the training (x_train, y_train) and test (x_test, y_test) sets and scale their features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (242, 14)\n",
      "Age                          int64\n",
      "Sex                          int64\n",
      "Chest_pain                   int64\n",
      "Resting_blood_pressure       int64\n",
      "Cholesterol                  int64\n",
      "Fasting_blood_sugar          int64\n",
      "ECG_results                  int64\n",
      "Maximum_heart_rate           int64\n",
      "Exercise_induced_angina      int64\n",
      "ST_depression              float64\n",
      "ST_slope                     int64\n",
      "Major_vessels                int64\n",
      "Thalassemia_types            int64\n",
      "Heart_attack                 int64\n",
      "dtype: object\n",
      "data shape: (61, 14)\n",
      "Age                          int64\n",
      "Sex                          int64\n",
      "Chest_pain                   int64\n",
      "Resting_blood_pressure       int64\n",
      "Cholesterol                  int64\n",
      "Fasting_blood_sugar          int64\n",
      "ECG_results                  int64\n",
      "Maximum_heart_rate           int64\n",
      "Exercise_induced_angina      int64\n",
      "ST_depression              float64\n",
      "ST_slope                     int64\n",
      "Major_vessels                int64\n",
      "Thalassemia_types            int64\n",
      "Heart_attack                 int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
    "\n",
    "prep = Preprocessor()\n",
    "x_train = prep.fit_transform(x_train)\n",
    "x_test = prep.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split the test set into test (x_test, y_test) and validation (x_val, y_val) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test: 14/30\n",
      "y_train: 134/242\n",
      "y_val: 17/31\n"
     ]
    }
   ],
   "source": [
    "x_test, x_val, y_test, y_val = train_test_split(x_test, y_test, test_size=0.5, random_state=5)\n",
    "print(f\"y_test: {sum(y_test)}/{len(y_test)}\")\n",
    "print(f\"y_train: {sum(y_train)}/{len(y_train)}\")\n",
    "print(f\"y_val: {sum(y_val)}/{len(y_val)}\")\n",
    "input_shape = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later use in HE, we save the different preprocessed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving x_test of shape (30, 27) in datasets/x_test.h5\n",
      "Saving y_test of shape (30,) in datasets/x_test.h5\n",
      "Saving x_train of shape (242, 27) in datasets/x_train.h5\n",
      "Saving y_train of shape (242,) in datasets/x_train.h5\n",
      "Saving x_val of shape (31, 27) in datasets/x_val.h5\n",
      "Saving y_val of shape (31,) in datasets/x_val.h5\n"
     ]
    }
   ],
   "source": [
    "def save_data_set(x, y, data_type, path, s=''):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    fname=os.path.join(path, f'x_{data_type}{s}.h5')\n",
    "    print(\"Saving x_{} of shape {} in {}\".format(data_type, x.shape, fname))\n",
    "    xf = h5py.File(fname, 'w')\n",
    "    xf.create_dataset('x_{}'.format(data_type), data=x)\n",
    "    xf.close()\n",
    "\n",
    "    print(\"Saving y_{} of shape {} in {}\".format(data_type, y.shape, fname))\n",
    "    yf = h5py.File(os.path.join(path, f'y_{data_type}{s}.h5'), 'w')\n",
    "    yf.create_dataset(f'y_{data_type}', data=y)\n",
    "    yf.close()\n",
    "\n",
    "datasets_dir = \"datasets/\"\n",
    "model_dir = \"model/\"\n",
    "\n",
    "save_data_set(x_test, y_test, data_type='test', path=datasets_dir)\n",
    "save_data_set(x_train, y_train, data_type='train', path=datasets_dir)\n",
    "save_data_set(x_val, y_val, data_type='val', path=datasets_dir)\n",
    "\n",
    "prep.save(os.path.join(model_dir, \"prep.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model\n",
    "\n",
    "The model has 3 layers: \n",
    "\n",
    "FC(50) --> Square activation --> FC(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-07 20:31:19.620742: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-06-07 20:31:19.620786: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-06-07 20:31:19.620831: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (lnx-mpc1-test): /proc/driver/nvidia/version does not exist\n",
      "2022-06-07 20:31:19.621275: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def square(x):\n",
    "    return x ** 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(50, input_shape=(input_shape,)))\n",
    "model.add(Activation(activation=square))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                1400      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,451\n",
      "Trainable params: 1,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "25/25 - 1s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 673ms/epoch - 27ms/step\n",
      "Epoch 2/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 55ms/epoch - 2ms/step\n",
      "Epoch 3/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 47ms/epoch - 2ms/step\n",
      "Epoch 4/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 49ms/epoch - 2ms/step\n",
      "Epoch 5/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 48ms/epoch - 2ms/step\n",
      "Epoch 6/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 50ms/epoch - 2ms/step\n",
      "Epoch 7/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 52ms/epoch - 2ms/step\n",
      "Epoch 8/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 52ms/epoch - 2ms/step\n",
      "Epoch 9/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 52ms/epoch - 2ms/step\n",
      "Epoch 10/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 50ms/epoch - 2ms/step\n",
      "Epoch 11/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 52ms/epoch - 2ms/step\n",
      "Epoch 12/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 49ms/epoch - 2ms/step\n",
      "Epoch 13/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 51ms/epoch - 2ms/step\n",
      "Epoch 14/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 51ms/epoch - 2ms/step\n",
      "Epoch 15/15\n",
      "25/25 - 0s - loss: 0.0000e+00 - accuracy: 0.4463 - val_loss: 0.0000e+00 - val_accuracy: 0.4516 - 49ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3d444a1730>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sum_squared_error(y_true, y_pred):\n",
    "    y_true = tf.cast(y_pred, tf.float32)\n",
    "    return K.sum(K.square(y_pred - y_true), axis=-1)\n",
    "\n",
    "loss_func = sum_squared_error\n",
    "optimizer_type = SGD(lr=0.01, momentum=0.9)  # Adam\n",
    "\n",
    "model.compile(loss=loss_func,optimizer=optimizer_type,metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "batch_size = 10\n",
    "epochs = 15\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_val, y_val),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For later use in HE, we save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weights to: model/nn_heart_disease_model_weights.h5\n"
     ]
    }
   ],
   "source": [
    "def save_weights(model, index, path):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    fname = os.path.join(path, \"nn_heart_disease_model_weights.h5\")\n",
    "    print(\"Saving weights to: \" + fname)\n",
    "    model.save_weights(fname)\n",
    "    s = model.to_json()\n",
    "\n",
    "    with open(os.path.join(path, f'nn_heart_disease_model.json'), 'w') as f:\n",
    "        f.write(s)\n",
    "\n",
    "save_weights(model, epochs, path=model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.000\n",
      "Test accuracy:53.333\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print(f'Test loss: {score[0]:.3f}')\n",
    "print(f'Test accuracy:{score[1] * 100:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the model for classifying cleartest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_vals = model.predict(x_test)\n",
    "y_pred_vals = tf.keras.activations.sigmoid(y_pred_vals).numpy()\n",
    "\n",
    "y_pred = (y_pred_vals > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.5044642857142857\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.94      0.68        16\n",
      "           1       0.50      0.07      0.12        14\n",
      "\n",
      "    accuracy                           0.53        30\n",
      "   macro avg       0.52      0.50      0.40        30\n",
      "weighted avg       0.52      0.53      0.42        30\n",
      "\n",
      "Confusion Matrix:\n",
      "[[15  1]\n",
      " [13  1]]\n"
     ]
    }
   ],
   "source": [
    "f, t, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "cm = metrics.confusion_matrix(y_test, y_pred)\n",
    "print(\"Score:\", metrics.auc(f, t))\n",
    "print(\"Classification report:\")\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the model for classifying encrypted data\n",
    "\n",
    "To run the model over encrypted samples with homomorphic encryption (HE), we first load the pyhelayers package and refer it to the directory \"output/\", where we saved the model and the relevant datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyhelayers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load test data and labels from the h5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(datasets_dir + \"x_test.h5\") as f:\n",
    "    x_test = np.array(f[\"x_test\"])\n",
    "with h5py.File(datasets_dir + \"y_test.h5\") as f:\n",
    "    y_test = np.array(f[\"y_test\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load a plain model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded plain model\n"
     ]
    }
   ],
   "source": [
    "nnp = pyhelayers.NeuralNetPlain()\n",
    "nnp.init_arch_from_json_file(model_dir + \"nn_heart_disease_model.json\")\n",
    "nnp.init_weights_from_hdf5_file(model_dir + \"nn_heart_disease_model_weights.h5\")\n",
    "print(\"loaded plain model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply automatic optimziations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = pyhelayers.DefaultContext()\n",
    "optimizer = pyhelayers.HeProfileOptimizer(nnp, context)\n",
    "optimizer.get_requirements().set_batch_size(16)\n",
    "profile = optimizer.get_optimized_profile(False)\n",
    "batch_size = profile.get_batch_size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the memory requirements of the context, we reduce the number of rotation keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf1=pyhelayers.PublicFunctions()\n",
    "pf1.rotate=pyhelayers.RotationSetType.CUSTOM_ROTATIONS\n",
    "pf1.set_rotation_steps([1,2])\n",
    "pf1.conjugate=False\n",
    "requirements = profile.requirement\n",
    "requirements.public_functions=pf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intialize the HE context with the optimized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HE Context ready. Batch size= 16\n"
     ]
    }
   ],
   "source": [
    "context.init(profile.requirement)\n",
    "print('HE Context ready. Batch size=',batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the HE context (w/ keys) size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size 8.189205169677734 MB\n"
     ]
    }
   ],
   "source": [
    "evalBuf=context.save_to_buffer();\n",
    "print('Size',len(evalBuf)/1024/1024,'MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encrypt the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Object (detailed printing not implemented yet)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = pyhelayers.NeuralNet(context)\n",
    "nn.encode_encrypt(nnp, profile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the encrypted model over batches of 16 records at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_samples = x_test.take(indices=range(0, batch_size), axis=0)\n",
    "labels = y_test.take(indices=range(0, batch_size), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encrypt input samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = nn.encode_encrypt_input(plain_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we perform inference of the 16 samples under encryption "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=nn.predict(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plaintext results\n",
    "\n",
    "Decrypting the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_predictions = nn.decrypt_decode_output(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "classification results\n",
      "=========================================\n",
      "Label: Healthy, Prediction: Healthy\n",
      "Label: Healthy, Prediction: Healthy\n",
      "Label: Should talk with a Dr., Prediction: Healthy\n",
      "Label: Should talk with a Dr., Prediction: Healthy\n",
      "Label: Should talk with a Dr., Prediction: Healthy\n",
      "Label: Healthy, Prediction: Healthy\n",
      "Label: Healthy, Prediction: Healthy\n",
      "Label: Healthy, Prediction: Healthy\n",
      "Label: Healthy, Prediction: Healthy\n",
      "Label: Should talk with a Dr., Prediction: Healthy\n",
      "Label: Healthy, Prediction: Healthy\n",
      "Label: Healthy, Prediction: Healthy\n",
      "Label: Should talk with a Dr., Prediction: Healthy\n",
      "Label: Healthy, Prediction: Healthy\n",
      "Label: Should talk with a Dr., Prediction: Healthy\n",
      "Label: Healthy, Prediction: Healthy\n"
     ]
    }
   ],
   "source": [
    "print('\\nclassification results')\n",
    "print('=========================================')\n",
    "for label,pred in zip(labels,plain_predictions):\n",
    "    print('Label:',('Healthy' if label==0 else 'Should talk with a Dr.'),end=', ')\n",
    "    print('Prediction:',('Healthy' if pred[0]<0.5 else 'Should talk with a Dr.'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fhe-py38-env",
   "language": "python",
   "name": "fhe-py38-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
